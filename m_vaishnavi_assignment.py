# -*- coding: utf-8 -*-
"""M Vaishnavi assignment

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SYFoNxBF49T2oEp4r9PM4q1ukUG59MLj
"""

import requests
from bs4 import BeautifulSoup

base_url = "https://www.amazon.in"
search_term = "bags"
pages_to_scrape = 20

for page in range(1, pages_to_scrape + 1):
    url = f"https://www.amazon.in/s?k={search_term}&page={page}"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    results = soup.find_all("div", {"data-component-type": "s-search-result"})

    for result in results:
        product_url = base_url + result.find("a", class_="a-link-normal").get("href")
        product_name = result.find("span", class_="a-size-medium").text.strip()
        product_price = result.find("span", class_="a-price-whole").text.strip()
        rating = result.find("span", class_="a-icon-alt").text.strip().split()[0]
        reviews = result.find("span", class_="a-size-base").text.strip()

        print("Product URL:", product_url)
        print("Product Name:", product_name)
        print("Product Price:", product_price)
        print("Rating:", rating)
        print("Number of Reviews:", reviews)
        print("------------------------")

import csv
import requests
from bs4 import BeautifulSoup

base_url = "https://www.amazon.in"
search_term = "bags"
pages_to_scrape = 20
products_data = []

for page in range(1, pages_to_scrape + 1):
    url = f"https://www.amazon.in/s?k={search_term}&page={page}"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    results = soup.find_all("div", {"data-component-type": "s-search-result"})

    for result in results:
        product_url = base_url + result.find("a", class_="a-link-normal").get("href")
        product_name = result.find("span", class_="a-size-medium").text.strip()
        product_price = result.find("span", class_="a-price-whole").text.strip()
        rating = result.find("span", class_="a-icon-alt").text.strip().split()[0]
        reviews = result.find("span", class_="a-size-base").text.strip()

        # Scraping additional information from the product URL
        product_response = requests.get(product_url)
        product_soup = BeautifulSoup(product_response.content, "html.parser")

        description_element = product_soup.find("div", id="feature-bullets")
        description = description_element.text.strip() if description_element else ""

        asin_element = product_soup.find("th", text="ASIN")
        asin = asin_element.find_next("td").text.strip() if asin_element else ""

        product_description_element = product_soup.find("div", id="productDescription")
        product_description = product_description_element.text.strip() if product_description_element else ""

        manufacturer_element = product_soup.find("a", id="bylineInfo")
        manufacturer = manufacturer_element.text.strip() if manufacturer_element else ""

        product_data = {
            "Product URL": product_url,
            "Product Name": product_name,
            "Product Price": product_price,
            "Rating": rating,
            "Number of Reviews": reviews,
            "Description": description,
            "ASIN": asin,
            "Product Description": product_description,
            "Manufacturer": manufacturer
        }

        products_data.append(product_data)

    print(f"Scraped page {page} of {pages_to_scrape}")

# Exporting the data to a CSV file
csv_filename = "product_data.csv"
csv_fields = list(products_data[0].keys())

with open(csv_filename, mode="w", newline="", encoding="utf-8") as file:
    writer = csv.DictWriter(file, fieldnames=csv_fields)
    writer.writeheader()
    writer.writerows(products_data)

print(f"Data exported to '{csv_filename}'")